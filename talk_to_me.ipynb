{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"unicamp.png\" width=\"150\" height=\"150\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talk to Me\n",
    "\n",
    "Este notebook disponibiliza um código para interagir com a solução de FAQ. Para garantir que todos os requisitos estejam instalados execute o código a seguir:\n",
    "\n",
    "<i>pip install -r requirements.txt</i>\n",
    "\n",
    "Após, execute todas as células do notebook em ordem. A última executa um loop que recebe a pergunta como entrada. Para sair digite \"exit\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default\n",
    "import os\n",
    "\n",
    "# Numerical and IO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# NLP\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(file: str, data):\n",
    "    \n",
    "    folder = 'pickles/'\n",
    "    with open(f'{folder}{file}.pickle', 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load(file: str):\n",
    "\n",
    "    folder = 'pickles/'\n",
    "    with open(f'{folder}{file}.pickle', 'rb') as handle:\n",
    "        pick = pickle.load(handle)\n",
    "    \n",
    "    return pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    \n",
    "    emb_model = load('doc2vec')\n",
    "    \n",
    "    sub_model = load('sub_model')\n",
    "    que_models = {}\n",
    "    \n",
    "    for i in range(1, 17):\n",
    "        que_models[str(i)] = load(f'{i}_que_model')\n",
    "    \n",
    "    return emb_model, sub_model, que_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_classes():\n",
    "    \n",
    "    class_sub = load('class_sub')\n",
    "    class_que = load('class_que')\n",
    "    \n",
    "    return class_sub, class_que"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, emb_model):\n",
    "    \n",
    "    tokenized = word_tokenize(text.lower())\n",
    "    return emb_model.infer_vector(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(emb_model, sub_model, que_models, class_sub, class_que, question):\n",
    "    \n",
    "    # Get embedding\n",
    "    q = get_embedding(question, emb_model)\n",
    "    \n",
    "    # Predict goal\n",
    "    pred = sub_model.predict([q])\n",
    "    \n",
    "    # Predict answer\n",
    "    ans = que_models[pred[0]].predict([q])\n",
    "    \n",
    "    return f'Goal: {class_sub[pred[0]]} / Answer: {class_que[ans[0]]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stuff\n",
    "emb_model, sub_model, que_models = load_models()\n",
    "class_sub, class_que = load_classes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello, welcome to Sustainable Development Goals FAQ from United Nations\")\n",
    "print(\"Type 'exit' to leave\")\n",
    "\n",
    "while True:\n",
    "    \n",
    "    question = input('Type your question: ')\n",
    "    if question == 'exit':\n",
    "        break\n",
    "        \n",
    "    answer = get_answer(emb_model, \n",
    "               sub_model, \n",
    "               que_models, \n",
    "               class_sub,\n",
    "               class_que,\n",
    "               question)\n",
    "    \n",
    "    print(answer)\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
